{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphAttentionCNN for pytorch\n",
    "###### for ABIDE study, by Hyeokjin Kwon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 . Import the modules / Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import smogn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_sparse_spd_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold,StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data.dataset\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import rankdata\n",
    "import copy\n",
    "from sqrtm import sqrtm\n",
    "\n",
    "from MachineLearningLibraryHyeokjin import *\n",
    "from FunctionalNetworkLibraryHyeokjin import *\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "class E2EBlock(torch.nn.Module):\n",
    "    '''E2Eblock.'''\n",
    "\n",
    "    def __init__(self, in_planes, planes,example,bias=False):\n",
    "        super(E2EBlock, self).__init__()\n",
    "        self.d = example.size(3)\n",
    "        self.cnn1 = torch.nn.Conv2d(in_planes,planes,(1,self.d),bias=bias)\n",
    "        torch.nn.init.kaiming_normal_(self.cnn1.weight)\n",
    "        self.cnn2 = torch.nn.Conv2d(in_planes,planes,(self.d,1),bias=bias)\n",
    "        torch.nn.init.kaiming_normal_(self.cnn2.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = self.cnn1(x)\n",
    "        b = self.cnn2(x)\n",
    "        return torch.cat([a]*self.d,3)+torch.cat([b]*self.d,2)\n",
    "\n",
    "# Normalizing the adjacency matrix by a degree matrix\n",
    "def norm_g(g):\n",
    "    degrees = torch.sum(g, 1)\n",
    "    g = g / degrees\n",
    "    return g\n",
    "\n",
    "# GCN model for batch graph data\n",
    "# H1 = act( norm(G)*H0*W )\n",
    "class batchedGCN(torch.nn.Module):\n",
    "    def __init__(self, in_planes, planes):\n",
    "        super(batchedGCN,self).__init__()\n",
    "        # W\n",
    "        self.proj = torch.nn.Linear(in_planes, planes)\n",
    "        \n",
    "    def forward(self,h,g):\n",
    "        # h&g - batch x channel x node x node \n",
    "        h_ones, g_ones = torch.split(h,1,dim=0),torch.split(g,1,dim=0)\n",
    "        o_hs = []\n",
    "        for batch in range(len(h_ones)):\n",
    "            # h_one&g_one - node x node\n",
    "            h_one, g_one = h_ones[batch].squeeze(0).squeeze(0),g_ones[batch].squeeze(0).squeeze(0)\n",
    "            \n",
    "            # preprocessing : g_one\n",
    "            g_one = (g_one > 0).float() # binarization\n",
    "            g = norm_g(g_one) # normalization\n",
    "            \n",
    "            # calculate : norm(G)*H0*W\n",
    "            h = torch.matmul(g,h_one)\n",
    "            h = self.proj(h)\n",
    "            o_hs.append(h)\n",
    "            \n",
    "        hs = torch.stack(o_hs,0).unsqueeze(1) # stack along the batch & add the channel\n",
    "        \n",
    "        return hs\n",
    "\n",
    "# top-k pooling using the attention score\n",
    "class top_k_pool(torch.nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super(top_k_pool,self).__init__()\n",
    "        # Nodes which have an upper k score will be survived\n",
    "        self.k = k\n",
    "        \n",
    "    def forward(self,h,g,scores):\n",
    "        # h&g - batch x channel x node x node \n",
    "        # score - batch x 1 x node x 1\n",
    "        h_ones, g_ones, score_ones = torch.split(h,1,dim=0),torch.split(g,1,dim=0),torch.split(scores,1,dim=0)\n",
    "        h_pool,g_pool,score_pool = [],[],[]\n",
    "        for batch in range(len(h_ones)):\n",
    "            # h_one&g_one - node x node\n",
    "            h_one, g_one = h_ones[batch].squeeze(0).squeeze(0),g_ones[batch].squeeze(0).squeeze(0)\n",
    "            \n",
    "            # score - node x 1\n",
    "            score = score_ones[batch].squeeze(0).squeeze(0)\n",
    "            values, idx = torch.topk(score.squeeze(),self.k,dim=0)\n",
    "            \n",
    "            # top-k selection\n",
    "            new_h = h_one[idx,:] # k x channel\n",
    "            new_g = g_one[idx,:] \n",
    "            new_g = new_g[:,idx] # k x k\n",
    "            new_score = score[idx] # k x 1\n",
    "            \n",
    "            h_pool.append(new_h)\n",
    "            g_pool.append(new_g)\n",
    "            score_pool.append(new_score)\n",
    "        \n",
    "        # stack along the batch & add the channel\n",
    "        hs, gs = torch.stack(h_pool,0).unsqueeze(1), torch.stack(g_pool,0).unsqueeze(1)\n",
    "        ss = torch.stack(score_pool,0).unsqueeze(1)\n",
    "        \n",
    "        return hs, gs, ss\n",
    "    \n",
    "def ReadOut(h):\n",
    "    # input h : batch x channel x node x 1\n",
    "    # maxpooling : batch x channel x 1\n",
    "    max_out, _ = torch.max(h,dim=2)\n",
    "    # meanpooling : batch x channel x 1\n",
    "    mean_out = torch.mean(h,dim=2)\n",
    "    # concatenate : batch x 2*channel x 1\n",
    "    new_h = torch.cat((max_out,mean_out),dim=1)\n",
    "    \n",
    "    return new_h\n",
    "    \n",
    "# main model\n",
    "class GaGCN(torch.nn.Module):\n",
    "    def __init__(self, example):\n",
    "        super(GaGCN, self).__init__()\n",
    "        # flexible shape\n",
    "        self.in_planes = example.size(1)\n",
    "        self.d = example.size(3)\n",
    "        \n",
    "        # \n",
    "        self.lReLu = torch.nn.LeakyReLU(0.33)\n",
    "        self.ReLu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        # init : feature extractor\n",
    "        self.e2econv1 = E2EBlock(1,128,example,bias=True)\n",
    "        self.e2econv2 = E2EBlock(128,256,example,bias=True)\n",
    "        self.E2N = torch.nn.Conv2d(256,8,(1,self.d))\n",
    "        torch.nn.init.kaiming_normal_(self.E2N.weight)\n",
    "        \n",
    "        # init : attention module\n",
    "        self.attention11 = batchedGCN(8,1)\n",
    "        self.attention12 = batchedGCN(1,1)\n",
    "        \n",
    "        self.gcn1 = batchedGCN(8,8)\n",
    "        self.attention21 = batchedGCN(8,1)\n",
    "        self.attention22 = batchedGCN(1,1)\n",
    "        \n",
    "        # top-k pooling layer\n",
    "        self.topkpool1 = top_k_pool(int(self.d/2))\n",
    "        self.topkpool2 = top_k_pool(int(self.d/4))\n",
    "        \n",
    "        # init : predictor (FCN)\n",
    "        self.dense1 = torch.nn.Linear(16,128)\n",
    "        self.dense2 = torch.nn.Linear(128,64)\n",
    "        self.dense3 = torch.nn.Linear(64,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # feature extractor : E2E\n",
    "        out = self.lReLu(self.e2econv1(x))\n",
    "        out = self.lReLu(self.e2econv2(out))\n",
    "        out = self.lReLu(self.E2N(out))\n",
    "        \n",
    "        # Node attention score\n",
    "        weight = self.lReLu(self.attention11(out.permute(0,3,2,1),x))\n",
    "        weight = self.sigmoid(self.attention12(weight,x))\n",
    "        \n",
    "        # top k pooling\n",
    "        out, x, weight = self.topkpool1(out.permute(0,3,2,1),x,weight)\n",
    "        out = out.permute(0,3,2,1)\n",
    "        out = out*weight + out\n",
    "        sum1 = ReadOut(out)\n",
    "        \n",
    "        # gcn layer1\n",
    "        out = self.gcn1(out.permute(0,3,2,1),x).permute(0,3,2,1)\n",
    "        \n",
    "        # Node attention score\n",
    "        weight = self.lReLu(self.attention21(out.permute(0,3,2,1),x))\n",
    "        weight = self.sigmoid(self.attention22(weight,x))\n",
    "        \n",
    "        # top k pooling\n",
    "        out, x, weight = self.topkpool2(out.permute(0,3,2,1),x,weight)\n",
    "        out = out.permute(0,3,2,1)\n",
    "        out = out*weight + out\n",
    "        sum2 = ReadOut(out)\n",
    "        \n",
    "        # summary the readout results\n",
    "        out = torch.add(sum1,sum2)\n",
    "        \n",
    "        # predictor\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(self.lReLu(self.dense1(out)))\n",
    "        out = self.dropout(self.lReLu(self.dense2(out)))\n",
    "        out = self.dense3(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "def init_weights_he(m):\n",
    "    #https://keras.io/initializers/#he_uniform\n",
    "    print(m)\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        fan_in = net.dense1.in_features\n",
    "        he_lim = np.sqrt(6) / fan_in\n",
    "        m.weight.data.uniform_(-he_lim,he_lim)\n",
    "        \n",
    "def mse_loss(input, target):\n",
    "    return torch.sum((input - target) ** 2)\n",
    "\n",
    "def weighted_mse_loss(input, target, weight):\n",
    "    return torch.sum(weight * (input - target) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 . Custom dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, transform=False,\n",
    "        mode=\"train\",\n",
    "        K=0, num_folds=5, val_split=0.1, stratify=False,\n",
    "        random_state=11,\n",
    "        SMOGN=False\n",
    "    ):\n",
    "        # check the parced mode\n",
    "        assert mode in [\"train\", \"validation\", \"test\"]\n",
    "        # parcing...\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.K = K\n",
    "        self.num_folds = num_folds\n",
    "        self.val_split = val_split\n",
    "        self.stratify = stratify\n",
    "        self.random_state = random_state\n",
    "        self.SMOGN = SMOGN\n",
    "        \n",
    "        # read the data \n",
    "        # demo\n",
    "        demopath = os.path.abspath('/nasdata2/khj/ai/abide_demo991')\n",
    "        demo = pd.read_table(demopath,sep=' ') \n",
    "        # graph network data - X \n",
    "        demo_fil = demo[np.isfinite(demo['ADI_R_SOCIAL_TOTAL_A'])]\n",
    "        demo_fil = demo_fil.drop(demo_fil[demo_fil['SUB_ID'] == 29206].index)\n",
    "        demo_fil = demo_fil.drop(demo_fil[demo_fil['SUB_ID'] == 28702].index)\n",
    "        # all_mats : N x P x P (N : # of the samples, P : # of the nodes)\n",
    "        # y : N x 1 \n",
    "        all_mats, y  = get_netmat(\n",
    "            parcel = 'SHEN',\n",
    "            fc_mode = 'Tikhonov',\n",
    "            ref_mode = 'mean',\n",
    "            mat_path = '/nasdata2/khj/abide/SMT',\n",
    "            demo=demo_fil,\n",
    "            subj_key='SUB_ID',\n",
    "            phenotype='ADI_R_SOCIAL_TOTAL_A'\n",
    "        )\n",
    "        \n",
    "        # matrix to vector (Necessary for combat, normalization, and so on...)\n",
    "        num_nodes = np.shape(all_mats)[-1]\n",
    "        numel = int((1+num_nodes)*num_nodes/2) # of upper triangular elements\n",
    "        X_vec = np.zeros((np.shape(all_mats)[0],numel)) # initialization\n",
    "        \n",
    "        for s in range(np.shape(all_mats)[0]):\n",
    "            # keep 5% sparsity : proportional thresholding will be performed\n",
    "            tmp = make_sparse_matrix(mat=all_mats[s,:,:],thr=95,mode='prop') \n",
    "            # standardized to 0 mean and 1 standart deviation\n",
    "            ztmp = normalize_netmat(tmp,mode='z')\n",
    "            # extract the upper triangular part from the network matrix\n",
    "            X_vec[s,:] = ztmp[np.triu_indices(np.shape(all_mats)[-1])]\n",
    "\n",
    "        # covariates : age, and sex\n",
    "        covs = np.zeros((np.shape(all_mats)[0],2))\n",
    "        covs[:,0] = demo_fil['AGE_AT_SCAN'].to_numpy()\n",
    "        covs[:,1] = demo_fil['SEX'].to_numpy()\n",
    "        #covs[:,2] = demo_fil['SITE_ID_NUM'].to_numpy()\n",
    "        #X1_vec, y0 = decomf_covariates(X_vec=X_vec,y=y,mode='X1y0',covs=covs)\n",
    "        \n",
    "        # Combat harmonization \n",
    "        batch = demo_fil['SITE_ID_NUM'].to_numpy()\n",
    "        r = rankdata(batch, method='dense').reshape(batch.shape)\n",
    "        ranks = (r.max()+1) - r\n",
    "        X1_vec, y0 = runCombat(X=X_vec,batch=ranks,covs=covs), y\n",
    "        \n",
    "        # vec -> mat restoration\n",
    "        X1 = np.zeros((len(y0),num_nodes,num_nodes))  # initialization\n",
    "        for s in range(len(y0)):\n",
    "            X1[s,:,:] = triu_transition(input_ndarray=X1_vec[s,:],mode='vec2mat')\n",
    "            \n",
    "        #split data - train vs test\n",
    "        cnt_idx = np.arange(len(y0))\n",
    "        train_indices,validation_indices,test_indices = {},{},{}  # initialization\n",
    "        \n",
    "        cnt = 0\n",
    "        if self.stratify == False: # Do not considering the stratification\n",
    "            kf = KFold(n_splits=self.num_folds,shuffle=True,random_state=self.random_state)\n",
    "            for train_validation_idx, test_idx in kf.split(cnt_idx):\n",
    "                # split - train + validation\n",
    "                splits = train_test_split(train_validation_idx, shuffle=True,\n",
    "                                          random_state=self.random_state,\n",
    "                                          test_size=self.val_split)\n",
    "                train_indices[str(cnt)], validation_indices[str(cnt)] = splits[0], splits[1]\n",
    "                # test\n",
    "                test_indices[str(cnt)] = test_idx\n",
    "                cnt += 1   \n",
    "                \n",
    "        elif self.stratify: # Considering the stratification \n",
    "            # (e.g. recommended for imbalanced or skewed target distribution)\n",
    "            kf = StratifiedKFold(n_splits=self.num_folds,shuffle=True,random_state=self.random_state)\n",
    "            for train_validation_idx, test_idx in kf.split(cnt_idx,y0):\n",
    "                # quantizing the target variable to avoid extremely rare sample\n",
    "                # Here, we used the lower and upper quantile value, \n",
    "                # But user should set the value by target distribution of the own data\n",
    "                y_percentile = np.percentile( y0[train_validation_idx], [25,75] )\n",
    "                y0_quntized = np.searchsorted(y_percentile, y0[train_validation_idx])\n",
    "                # split\n",
    "                splits = train_test_split(train_validation_idx, shuffle=True,\n",
    "                                          random_state=self.random_state,\n",
    "                                          test_size=self.val_split,\n",
    "                                          stratify=y0_quntized)\n",
    "                train_indices[str(cnt)], validation_indices[str(cnt)] = splits[0], splits[1]\n",
    "                test_indices[str(cnt)] = test_idx\n",
    "                cnt += 1          \n",
    "                \n",
    "        else: print('Unknown option argument : stratify')\n",
    "                \n",
    "        # finally, return the split indices\n",
    "        if self.mode==\"train\":\n",
    "            sel = train_indices[str(self.K)]   \n",
    "        elif self.mode==\"validation\":\n",
    "            sel = validation_indices[str(self.K)] \n",
    "        elif mode==\"test\":\n",
    "            sel = test_indices[str(self.K)]\n",
    "        else:\n",
    "            sel = cnt_idx\n",
    "            \n",
    "        x_sel, y_sel = X1[sel,:,:], y0[sel].reshape(len(sel),1)\n",
    "        \n",
    "        # SMOGN(optional) \n",
    "        x_vec = np.zeros((np.shape(x_sel)[0],numel)) \n",
    "        if self.mode == 'train' and self.SMOGN == True: # only for the train phase\n",
    "            # train data were vectorized\n",
    "            for s in range(len(y_sel)):\n",
    "                tmp = x_sel[s,:,:]\n",
    "                x_vec[s,:] = tmp[np.triu_indices(num_nodes)]\n",
    "            # convert to the Pandas dataframe\n",
    "            x_df, y_df = pd.DataFrame(x_vec), pd.DataFrame(y_sel,columns=['y'])\n",
    "            xy_df = pd.concat([x_df,y_df],axis=1)\n",
    "            \n",
    "            # SMOGN\n",
    "            xy_smogn = smogn.smoter(data=xy_df,y='y')\n",
    "            x_vec_smogn = xy_smogn.drop('y',axis=1).to_numpy()\n",
    "            y_smogn = xy_smogn['y'].to_numpy()\n",
    "        \n",
    "            # postprocessing...\n",
    "            y = y_smogn.reshape(len(y_smogn),1)\n",
    "            x = np.zeros((len(y_smogn),np.shape(x_sel)[-1],np.shape(x_sel)[-1]))\n",
    "            for s in range(len(y)):\n",
    "                x[s,:,:] = triu_transition(input_ndarray=x_vec_smogn[s,:],mode='vec2mat')\n",
    "        else: \n",
    "            x, y = x_sel, y_sel\n",
    "            \n",
    "        # subject weights for weighted MSE loss - not used any more (2020.12.14)\n",
    "        med = np.median(y)\n",
    "        smooth = med\n",
    "        self.slice_weights = (np.abs(y-med)+smooth) / (np.sum(np.abs(y-med)+smooth))\n",
    "        \n",
    "        # TORCH TENSOR\n",
    "        self.sel = sel\n",
    "        self.X = torch.FloatTensor(np.expand_dims(x,1).astype(np.float32))\n",
    "        self.Y = torch.FloatTensor(y.astype(np.float32))\n",
    "        print(self.mode,self.X.shape,(self.Y.shape))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = [self.X[idx], self.Y[idx], self.slice_weights[idx]]\n",
    "        if self.transform:\n",
    "            sample[0] = self.transform(sample[0])\n",
    "        return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 . Train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gpu setting\n",
    "GPU_NUM = 7\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) \n",
    "print ('Current cuda device Number =', torch.cuda.current_device())\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print('Current cuda device Name =', torch.cuda.get_device_name(GPU_NUM))\n",
    "    print('Current cuda device Memory =', (torch.cuda.get_device_properties(GPU_NUM).total_memory))\n",
    "\n",
    "# parameters\n",
    "num_folds = 5 # of training folds\n",
    "nbepochs = 1000 # of epochs\n",
    "nb = 8 # of mini-batch\n",
    "lr = 0.00001 # learning-rate \n",
    "momentum = 0.9 # momentum for SGD\n",
    "wd = 0.0001 # weight-decay for SGD\n",
    "random_state = 1 # for replicability\n",
    "val_split = 0.1 # validation proportion \n",
    "\n",
    "# trained model will be saved at:\n",
    "save_path = '/nasdata2/khj/ai/BrainNetGCN7_final/'\n",
    "\n",
    "# K-fold cross validation - initialization\n",
    "allloss_train, allloss_val = {},{}\n",
    "allmae_train, allmae_val = {},{}\n",
    "allp_val, allr_val = {},{}\n",
    "allpreds,ally_true = {},{}\n",
    "\n",
    "# actual processing...< K / num_folds >...\n",
    "for K in range(num_folds):\n",
    "\n",
    "    # Call the dataloader\n",
    "    trainset = my_dataset(\n",
    "        mode=\"train\",\n",
    "        K=K, num_folds=num_folds, val_split=val_split, stratify=True, \n",
    "        random_state=random_state,\n",
    "        SMOGN=False\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=nb,shuffle=True)\n",
    "    valset = my_dataset(\n",
    "        mode=\"validation\",\n",
    "        K=K, num_folds=num_folds, val_split=val_split, stratify=True, \n",
    "        random_state=random_state,\n",
    "        SMOGN=False\n",
    "    )\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # network initialization\n",
    "    net = GaGCN(trainset.X)\n",
    "    if use_cuda:\n",
    "        net = net.to(device)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    net.apply(init_weights_he)\n",
    "\n",
    "    # Loss\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD(net.parameters(),lr=lr,momentum=momentum,nesterov=True,weight_decay=wd)\n",
    "\n",
    "    # Learning rate scheduler - cosine decay\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, nbepochs, eta_min=0, last_epoch=-1)\n",
    "\n",
    "    # statistics iitialization\n",
    "    allloss_train[str(K)] = []\n",
    "    allloss_val[str(K)] = []\n",
    "    allmae_val[str(K)] = []\n",
    "    allmae_train[str(K)] = []\n",
    "    allr_val[str(K)] = []\n",
    "    allp_val[str(K)] = []\n",
    "\n",
    "    # for an ealry stopping\n",
    "    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "    best_mae = 100000\n",
    "\n",
    "    # actual training\n",
    "    for epoch in range(nbepochs):\n",
    "        ## ======================================== train ======================================== ##\n",
    "        train_loss = 0\n",
    "        running_loss = 0.0\n",
    "        net.train()\n",
    "\n",
    "        preds = []\n",
    "        ytrue = []\n",
    "        # read the batch\n",
    "        for batch_idx, (inputs, targets, sample_weights) in enumerate(trainloader):\n",
    "            # set the variable \n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                sample_weights = sample_weights.to(device)\n",
    "\n",
    "            # optimizer preparation\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forwarding\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            #loss = weighted_mse_loss(input=outputs, target=targets, weight=sample_weights)\n",
    "\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=5.0, norm_type=2)\n",
    "            optimizer.step()\n",
    "\n",
    "            # stack the results\n",
    "            running_loss += loss.item()\n",
    "            preds.append(outputs.data.cpu().numpy())\n",
    "            ytrue.append(targets.data.cpu().numpy())\n",
    "\n",
    "        # calculate the average loss\n",
    "        loss_train = running_loss/batch_idx\n",
    "\n",
    "        # list --> numpy array\n",
    "        preds = np.vstack(preds)\n",
    "        y_true = np.vstack(ytrue)\n",
    "\n",
    "        # loss history\n",
    "        allloss_train[str(K)].append(loss_train)\n",
    "\n",
    "        # calculate the MAE and memorize it.\n",
    "        mae_train = mae(preds[:,0],y_true[:,0])\n",
    "        allmae_train[str(K)].append(mae_train)\n",
    "\n",
    "        # report the training phase...\n",
    "        print(\"Train Set : MAE for Engagement : %0.6f\" % (mae_train))\n",
    "\n",
    "        ## ======================================== validation ======================================== ##\n",
    "        net.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        preds = []\n",
    "        ytrue = []\n",
    "        for batch_idx, (inputs, targets,sample_weights) in enumerate(valloader):\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                sample_weights = sample_weights.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            #loss = weighted_mse_loss(input=outputs, target=targets, weight=sample_weights)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds.append(outputs.data.cpu().numpy())\n",
    "            ytrue.append(targets.data.cpu().numpy())\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        loss_val =  running_loss/batch_idx   \n",
    "        preds = np.vstack(preds)\n",
    "        y_true = np.vstack(ytrue)\n",
    "\n",
    "        allloss_val[str(K)].append(loss_val)\n",
    "\n",
    "        print(\"Fold %d Epoch %d\" % (K, epoch))\n",
    "        mae_val = mae(preds[:,0],y_true[:,0])\n",
    "        allmae_val[str(K)].append(mae_val)\n",
    "\n",
    "        # calculate the pearson correlation & corresponding p value\n",
    "        r_val, p_val = pearsonr(preds[:,0],y_true[:,0])\n",
    "        allr_val[str(K)].append(r_val)\n",
    "        allp_val[str(K)].append(p_val)\n",
    "\n",
    "        # report the validatoin phase...\n",
    "        print(\"Validation Set : MAE for Engagement : %0.6f\" % (mae_val))\n",
    "        print(\"               : Pearson r / p for Engagement : %0.4f / %0.4f\" % (r_val, p_val))\n",
    "\n",
    "        # for the early stopping, memorize the best model parameter w.r.t validation MAE\n",
    "        if mae_val < best_mae:\n",
    "            best_mae = mae_val\n",
    "            best_model_wts = copy.deepcopy(net.state_dict())\n",
    "        print('Best val MAE: {:4f}'.format(best_mae))\n",
    "\n",
    "        # after train&validation, learning rate will be updated\n",
    "        scheduler.step()\n",
    "\n",
    "    ## ======================================== test ======================================== ##\n",
    "    # TEST dataloader\n",
    "    testset = my_dataset(\n",
    "        mode=\"test\",\n",
    "        K=K, num_folds=num_folds, val_split=val_split, stratify=False,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "    # load the best model status\n",
    "    net.load_state_dict(best_model_wts)\n",
    "    net.eval()\n",
    "\n",
    "    # initialization\n",
    "    preds = []\n",
    "    ytrue = []\n",
    "    mae_test = 0\n",
    "    for batch_idx, (inputs, targets, sample_weights) in enumerate(testloader):\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        preds.append(outputs.data.cpu().numpy())\n",
    "        ytrue.append(targets.data.cpu().numpy())\n",
    "\n",
    "    preds = np.vstack(preds)\n",
    "    y_true = np.vstack(ytrue)\n",
    "\n",
    "    allpreds[str(K)] = preds[:,0]\n",
    "    ally_true[str(K)] = y_true[:,0]\n",
    "\n",
    "    mae_test = mae(allpreds[str(K)],ally_true[str(K)])\n",
    "    r_test, p_test = pearsonr(allpreds[str(K)],ally_true[str(K)])\n",
    "    print(\"Test Set : MAE for Engagement : %0.6f\" % (mae_test))\n",
    "    print(\"Test Set : Pearson r / p for Engagement : %0.4f / %0.4f\" % (r_test, p_test))\n",
    "\n",
    "    # save\n",
    "    mystring = 'ADIR_SOCIAL_FOLD' + str(K) + '_SMT' \n",
    "    filename_pt = save_path + mystring + \"_model.pt\"\n",
    "    filename_stats = save_path + mystring + \"_stats.npz\"\n",
    "    torch.save(net.state_dict(),filename_pt)\n",
    "    np.savez_compressed(filename_stats,val_losses = allloss_val,train_losses = allloss_train,mae_training = allmae_train,\n",
    "                  mae_val = allmae_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    plt.title('fold %d MAE' %(fold+1))\n",
    "    print('Train MAE = %.3f for fold %d' %( np.min(allmae_train[str(fold)]) , fold+1))\n",
    "    print('Validation MAE = %.3f for fold %d' %( np.min(allmae_val[str(fold)]) , fold+1))\n",
    "    plt.plot(allmae_train[str(fold)],label='train')\n",
    "    plt.plot(allmae_val[str(fold)],label='validation')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for fold in range(num_folds):\n",
    "    if fold == 0:\n",
    "        allpreds_array = allpreds[str(fold)]\n",
    "        alltrues_array = ally_true[str(fold)]\n",
    "        allmae_val_array = allmae_val[str(fold)]\n",
    "        allmae_train_array = allmae_train[str(fold)]\n",
    "    else:\n",
    "        allpreds_array = np.append(allpreds_array,allpreds[str(fold)])\n",
    "        alltrues_array = np.append(alltrues_array,ally_true[str(fold)])\n",
    "        allmae_val_array = np.append(allmae_val_array,allmae_val[str(fold)])\n",
    "        allmae_train_array = np.append(allmae_train_array,allmae_train[str(fold)])\n",
    "        \n",
    "print('Training MAE = %.3f' %(np.mean(allmae_train_array)))\n",
    "print('Validation MAE = %.3f' %(np.mean(allmae_val_array)))\n",
    "print('Test MAE = %.3f' %( mae(allpreds_array,alltrues_array)))\n",
    "\n",
    "sns.regplot(allpreds_array,alltrues_array)\n",
    "plt.plot(allpreds_array,allpreds_array,'k--',label='y=x')\n",
    "r, p = pearsonr(allpreds_array,alltrues_array)\n",
    "plt.xlabel('Predicts')\n",
    "plt.ylabel('True values')\n",
    "plt.title('Pearson R = %0.2f, p = %0.4f' % ( r,p) )\n",
    "plt.legend()\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asd_ai",
   "language": "python",
   "name": "asd_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
